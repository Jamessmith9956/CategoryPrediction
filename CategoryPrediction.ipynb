{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Plan\n",
    "1. Create a function to calculate the distance between descriptions, use that function to naively classify new descriptions. probably looks something like a search function.\n",
    "2. Grab a text classification model of hugging face and fine-tune it on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup requirements, which aren't loading in the venv for some reason\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open('data/Datas.csv') as file:\n",
    "    data = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data into training and testing sets in a stratified way, despite the minimum class only having 2 samples \n",
    "cat_count = data.value_counts('Category')\n",
    "subcat_count = data.value_counts(['Category', 'Sub_Category'])\n",
    "\n",
    "# handle the case where the minimum class has only 2 samples\n",
    "data = data.groupby('Category').filter(lambda x: len(x) > 2) # this method works, but my pandas is pretty rusty. \n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, stratify=data['Category'], random_state=42) # random_state for reproducibility\n",
    "train, val = train_test_split(train, test_size=0.3, stratify=train['Category'], random_state=77) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure distance \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# have a think about whether the data needs cleaning\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer() # use token_pattern to remove punctuation if needed\n",
    "tfidf.fit(train['Description']) # val set not included here \n",
    "tfidf_array = tfidf.transform(train['Description']).toarray()\n",
    "\n",
    "data_vectorized = pd.DataFrame(tfidf_array, \n",
    "                     columns=tfidf.get_feature_names_out(),\n",
    "                     index=train['Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic SciKitLearn Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "values = cosine_similarity(data_vectorized, data_vectorized) # dist() = x.y / (||x|| * ||y||) or sum(x * y) / (sqrt(sum(x^2)) * sqrt(sum(y^2)))\n",
    "similarities = pd.DataFrame(values, index=train.index, columns=train.index)\n",
    "\n",
    "# plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "Testing the variety of models, SVM and Naive Bayes worked the best on the raw data, while KNN and RF didn't work as well as expected.\n",
    "All models suffered from the distibution issue in the datset, only predicting the most common classes (food and entertainment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "def evaluate(Y_true, Y_pred, dataset_name=\"Train\"):\n",
    "    # function to evaluate the model\n",
    "    #confusion_matrix = confusion_matrix(val['Category'], val_hat)\n",
    "    #FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)  \n",
    "    #FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    #TP = np.diag(confusion_matrix)\n",
    "    #TN = confusion_matrix.sum() - (FP + FN + TP)\n",
    "    \n",
    "    # evaluate using the sklearn functions\n",
    "    accuracy = np.mean(accuracy_score(Y_true, Y_pred)) #(TP + TN) / (TP + FP + FN + TN)\n",
    "    recall = recall_score(Y_true, Y_pred, average='weighted') #TP / (TP + FN)\n",
    "    f1 = f1_score(Y_true, Y_pred, average='weighted')\n",
    "    \n",
    "    # because of our sparse data, precision is ill defined for many of our classes\n",
    "    \n",
    "    # plot\n",
    "    print(dataset_name,\" Accuracy:\", accuracy)\n",
    "    print(dataset_name,\" F1 Score\", f1)\n",
    "    \n",
    "def plot_confusion(Y_true, Y_pred, model, fig_name=\"confusion.png\"):\n",
    "    confusion = confusion_matrix(Y_true, Y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion, display_labels= model.classes_)\n",
    "    disp.plot(xticks_rotation=90)\n",
    "    plt.savefig('analysis/'+fig_name, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# setup KNN\n",
    "k = int(np.sqrt(train['Category'].count())) # sqrt(n) is a common choice for k\n",
    "knn = KNeighborsClassifier(n_neighbors=k, metric='cosine')\n",
    "knn.fit(data_vectorized.values, train['Category'])\n",
    "\n",
    "# predict\n",
    "val_hat = knn.predict(tfidf.transform(val['Description']).toarray())\n",
    "\n",
    "# evaluate\n",
    "evaluate(train['Category'], knn.predict(tfidf.transform(train['Description']).toarray()), \"Test\")\n",
    "evaluate(val['Category'], val_hat, \"Val\")\n",
    "plot_confusion(val['Category'], val_hat, knn, \"confusion_matricies/knn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# setup RF\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(data_vectorized.values, train['Category'])\n",
    "\n",
    "# predict\n",
    "train_hat = rf.predict(tfidf.transform(train['Description']).toarray())\n",
    "val_hat = rf.predict(tfidf.transform(val['Description']).toarray())\n",
    "\n",
    "# evaluate\n",
    "evaluate(train['Category'], train_hat, \"Test\")\n",
    "evaluate(val['Category'], val_hat, \"Val\")\n",
    "plot_confusion(val['Category'], val_hat, rf, \"confusion_matricies/rf.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# setup NB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(data_vectorized.values, train['Category'])\n",
    "\n",
    "# predict\n",
    "train_hat = nb.predict(tfidf.transform(train['Description']).toarray())\n",
    "val_hat = nb.predict(tfidf.transform(val['Description']).toarray())\n",
    "\n",
    "# evaluate\n",
    "evaluate(train['Category'], train_hat, \"Test\")\n",
    "evaluate(val['Category'], val_hat, \"Val\")\n",
    "plot_confusion(val['Category'], val_hat, nb, \"confusion_matricies/nb.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# prepare data labels to be numerical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(train['Category'])\n",
    "train_Y = le.transform(train['Category'])\n",
    "val_Y = le.transform(val['Category'])\n",
    "\n",
    "# setup SVM\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(data_vectorized.values, train_Y)\n",
    "\n",
    "# predict\n",
    "train_hat = svm.predict(tfidf.transform(train['Description']).toarray())\n",
    "val_hat = svm.predict(tfidf.transform(val['Description']).toarray())\n",
    "\n",
    "# evaluate\n",
    "evaluate(train_Y, train_hat, \"Test\")\n",
    "evaluate(val_Y, val_hat, \"Val\")\n",
    "plot_confusion(val_Y, val_hat, svm, \"confusion_matricies/svm.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
